{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Global Radio Explorer Telescope Welcome to the documentation for the Global Radio Explorer Telescope! In the tabs above, you'll find the documentation for the hardware and software architectures as well as guides for construction and setup. These docs are a WIP and once we deploy, contributions are welcome!","title":"Home"},{"location":"#the-global-radio-explorer-telescope","text":"Welcome to the documentation for the Global Radio Explorer Telescope! In the tabs above, you'll find the documentation for the hardware and software architectures as well as guides for construction and setup. These docs are a WIP and once we deploy, contributions are welcome!","title":"The Global Radio Explorer Telescope"},{"location":"about/","text":"The Global Radio Telescope is an new low-cost radio telescope designed to be an all-sky monitor for bright radio bursts. Building on the success of STARE2, we will search for fast radio bursts (FRBs) emitted from Galactic magnetars as well as bursts from nearby galaxies. GReX will search down to ten microseconds time resolution, allowing us to find new super giant radio pulses from Milky Way pulsars and study their broadband emission. The proposed instrument will employ ultra-wide band (0.7-2 GHz) feeds coupled to a high performance (receiver temperature 10 K) low noise amplifier (LNA) originally developed for the DSA-110 and DSA-2000 projects. In GReX Phase I (GReX-I), unit systems will be deployed at Owens Valley Radio Observatory (OVRO) and Big Smoky Valley, Nevada. Phase II will expand the array, placing feeds in India, Australia, and elsewhere in order to build up to continuous coverage of nearly 4\u03c0 steradians and to increase our exposure to the Galactic plane. We model the local magnetar population to forecast for GReX, finding the improved sensitivity and increased exposure to the Galactic plane could lead to dozens of FRB-like bursts per year. For more information, read our paper here ! Team Project Scientist - Dr. Liam Connor, PhD Project Engineer - Kiran Shila, MSEE","title":"About"},{"location":"about/#team","text":"Project Scientist - Dr. Liam Connor, PhD Project Engineer - Kiran Shila, MSEE","title":"Team"},{"location":"hardware/assembly/","text":"Assembly Guide TODO! Once we have the complete pacakge, write a step by step guide for assembly and installation","title":"Assembly Guide"},{"location":"hardware/assembly/#assembly-guide","text":"TODO! Once we have the complete pacakge, write a step by step guide for assembly and installation","title":"Assembly Guide"},{"location":"hardware/box/","text":"The Box TODO! Once the box is done, document cables, routing, etc.","title":"The Box"},{"location":"hardware/box/#the-box","text":"TODO! Once the box is done, document cables, routing, etc.","title":"The Box"},{"location":"hardware/feed/","text":"Feed Antenna TODO! Jonas info","title":"Feed Antenna"},{"location":"hardware/feed/#feed-antenna","text":"TODO! Jonas info","title":"Feed Antenna"},{"location":"hardware/fem/","text":"Frontend Module The frontend module (FEM) is a device that performs the analog signal processing after the LNAs. This includes filtering, downconversion, and amplification. Additionally, this module provides rudimentary monitor and control support. Bare PCB Completed Module Hardware Design The hardware design itself is implemented in the free KiCAD program and is available here . To manufacture from gerber files, the stackup needs to be JLC7628 from JLCPCB. The current hardware uses ENIG to help reflow of the fine-pitch components. Schematics BOM Case Firmware Design The RF hardware mostly operates without the intervention of any software. The only step required to use the RF hardware is to set the valid attenuation level, which defaults to 0 dB. As such, the primary goal of the digital section of the FEM is to perform Monitor and Control (MnC). MnC is achieved via an 115200 baud 3.3V UART interface on the main connector. The firmware design is carried out in the Rust programming language, and whose source can be found here . Monitor Every 1 second on UART (115200 baud), JSON payload of monitor data is sent out with the following schema { \"boardTemp\" : 29.6 , \"voltages\" : { \"rawInput\" : 6.2 , \"analog\" : 4.9 , \"lnaOne\" : 5.3 , \"lnaTwo\" : 5.3 }, \"currents\" : { \"rawInput\" : 0.723 , \"analog\" : 0.53 , \"lnaOne\" : 0.053 , \"lnaTwo\" : 0.052 }, \"ifPower\" : { \"channelOne\" : -0.3 , \"channelTwo\" : -2.1 }, \"control\" : { \"calOne\" : false , \"calTwo\" : false , \"lnaOnePowered\" : true , \"lnaTwoPowered\" : true , \"attenuationLevel\" : 3 , \"ifPowerThreshold\" : -10 } } Control The control payload must be a complete JSON object of the following form: { \"control\" : { \"calOne\" : false , \"calTwo\" : false , \"lnaOnePowered\" : true , \"lnaTwoPowered\" : true , \"attenuationLevel\" : 3 , \"ifPowerThreshold\" : -10 } } Over UART, there is control for enabling/disabling the calibration output, the LNA bias, and the interstage IF attenuator. For the digital attenuator, there are four levels (0-3), representing 0, 4, 8, 12 dB. This attenuator is to maximize the dynamic range of the ADC and can be set for environmental RFI levels. Physical Interface There are eight LEDs on the front panel. Four red LEDs to indicate power statess, two blue LEDs for serial activity, and two green LEDs for system status. The green LEDs will be enabled when the IF power is at a nominal level and will flash when the calibration signal is enabled.","title":"Fronend Module"},{"location":"hardware/fem/#frontend-module","text":"The frontend module (FEM) is a device that performs the analog signal processing after the LNAs. This includes filtering, downconversion, and amplification. Additionally, this module provides rudimentary monitor and control support. Bare PCB Completed Module","title":"Frontend Module"},{"location":"hardware/fem/#hardware-design","text":"The hardware design itself is implemented in the free KiCAD program and is available here . To manufacture from gerber files, the stackup needs to be JLC7628 from JLCPCB. The current hardware uses ENIG to help reflow of the fine-pitch components. Schematics BOM Case","title":"Hardware Design"},{"location":"hardware/fem/#firmware-design","text":"The RF hardware mostly operates without the intervention of any software. The only step required to use the RF hardware is to set the valid attenuation level, which defaults to 0 dB. As such, the primary goal of the digital section of the FEM is to perform Monitor and Control (MnC). MnC is achieved via an 115200 baud 3.3V UART interface on the main connector. The firmware design is carried out in the Rust programming language, and whose source can be found here .","title":"Firmware Design"},{"location":"hardware/fem/#monitor","text":"Every 1 second on UART (115200 baud), JSON payload of monitor data is sent out with the following schema { \"boardTemp\" : 29.6 , \"voltages\" : { \"rawInput\" : 6.2 , \"analog\" : 4.9 , \"lnaOne\" : 5.3 , \"lnaTwo\" : 5.3 }, \"currents\" : { \"rawInput\" : 0.723 , \"analog\" : 0.53 , \"lnaOne\" : 0.053 , \"lnaTwo\" : 0.052 }, \"ifPower\" : { \"channelOne\" : -0.3 , \"channelTwo\" : -2.1 }, \"control\" : { \"calOne\" : false , \"calTwo\" : false , \"lnaOnePowered\" : true , \"lnaTwoPowered\" : true , \"attenuationLevel\" : 3 , \"ifPowerThreshold\" : -10 } }","title":"Monitor"},{"location":"hardware/fem/#control","text":"The control payload must be a complete JSON object of the following form: { \"control\" : { \"calOne\" : false , \"calTwo\" : false , \"lnaOnePowered\" : true , \"lnaTwoPowered\" : true , \"attenuationLevel\" : 3 , \"ifPowerThreshold\" : -10 } } Over UART, there is control for enabling/disabling the calibration output, the LNA bias, and the interstage IF attenuator. For the digital attenuator, there are four levels (0-3), representing 0, 4, 8, 12 dB. This attenuator is to maximize the dynamic range of the ADC and can be set for environmental RFI levels.","title":"Control"},{"location":"hardware/fem/#physical-interface","text":"There are eight LEDs on the front panel. Four red LEDs to indicate power statess, two blue LEDs for serial activity, and two green LEDs for system status. The green LEDs will be enabled when the IF power is at a nominal level and will flash when the calibration signal is enabled.","title":"Physical Interface"},{"location":"hardware/fpga/","text":"Digital Backend TODO! Describe hardware interfaces of SNAP.","title":"Digital Backend"},{"location":"hardware/fpga/#digital-backend","text":"TODO! Describe hardware interfaces of SNAP.","title":"Digital Backend"},{"location":"hardware/overview/","text":"Hardware Overview The GReX hardware system has several \"top level\" components, which constitute the entire system. These include the feed antenna and low noise amplifiers (LNA), the frontend module , the digital backend , and of course the server. The following diagrams lay out general overview of the interconnections. Showing them all at once would be a bit much, so they're broken down here into discrete kinds of signals. RF Signal Path flowchart TD A[Feed] B[FEM] C[SNAP] D[Server] A --> L1[LNA] A --> L2[LNA] L1 -->|H Pol| B L2 -->|V Pol| B subgraph The Box B -->|H Pol| C B -->|V Pol| C end C -->|10 GbE| D[Server] Power Distribution flowchart BT L1[LNA] L2[LNA] B[FEM] C[SNAP] S[Switching Supply] R[Linear Regulator] P[Raspberry Pi] M[Mains Power] V[Synthesizer] G[GPS Receiver] M -->|120-240V AC| S subgraph The Box R -->|6.5V DC| V P -->|\"5V DC (USB)\"| G S -->|12V DC| C S -->|12V DC| R R -->|6.5V DC| B C -->|5V DC| P end B --->|5.5V DC| L1 B --->|5.5V DC| L2 Clocks, References and Timing flowchart BT B[FEM] V[Synthesizer] G[GPS Receiver] S[SNAP] A[GPS Antenna] subgraph The Box G -->|10 MHz| V G -->|PPS| S V -->|500 MHz| S V -->|1030 MHz| B end A --> G Monitor and Control flowchart TB B[FEM] S[SNAP] P[Raspberry Pi] D[Server] subgraph The Box P <-->|UART| B S <-->|GPIO| P end P <--->|1 GbE| D","title":"Overview"},{"location":"hardware/overview/#hardware-overview","text":"The GReX hardware system has several \"top level\" components, which constitute the entire system. These include the feed antenna and low noise amplifiers (LNA), the frontend module , the digital backend , and of course the server. The following diagrams lay out general overview of the interconnections. Showing them all at once would be a bit much, so they're broken down here into discrete kinds of signals.","title":"Hardware Overview"},{"location":"hardware/overview/#rf-signal-path","text":"flowchart TD A[Feed] B[FEM] C[SNAP] D[Server] A --> L1[LNA] A --> L2[LNA] L1 -->|H Pol| B L2 -->|V Pol| B subgraph The Box B -->|H Pol| C B -->|V Pol| C end C -->|10 GbE| D[Server]","title":"RF Signal Path"},{"location":"hardware/overview/#power-distribution","text":"flowchart BT L1[LNA] L2[LNA] B[FEM] C[SNAP] S[Switching Supply] R[Linear Regulator] P[Raspberry Pi] M[Mains Power] V[Synthesizer] G[GPS Receiver] M -->|120-240V AC| S subgraph The Box R -->|6.5V DC| V P -->|\"5V DC (USB)\"| G S -->|12V DC| C S -->|12V DC| R R -->|6.5V DC| B C -->|5V DC| P end B --->|5.5V DC| L1 B --->|5.5V DC| L2","title":"Power Distribution"},{"location":"hardware/overview/#clocks-references-and-timing","text":"flowchart BT B[FEM] V[Synthesizer] G[GPS Receiver] S[SNAP] A[GPS Antenna] subgraph The Box G -->|10 MHz| V G -->|PPS| S V -->|500 MHz| S V -->|1030 MHz| B end A --> G","title":"Clocks, References and Timing"},{"location":"hardware/overview/#monitor-and-control","text":"flowchart TB B[FEM] S[SNAP] P[Raspberry Pi] D[Server] subgraph The Box P <-->|UART| B S <-->|GPIO| P end P <--->|1 GbE| D","title":"Monitor and Control"},{"location":"software/guix/","text":"Pipeline Modules and Guix Guix is a functional package manager and tool to instantiate and manage Unix-like operating systems. By functional, Guix defines packages through a purely functional deployment model in which every build is deterministic and is a pure function of the package's \"inputs\" or dependencies. This solves the problem of dependency hell and reproducability. For GReX, many of our software modules and components exist as forks of preexisting software as well as some custom code. To ensure all of these components work together in harmony, we'll host a guix channel that provides the build recipes for our software here . Most of this software, however, relies on the non-free CUDA runtime. Note As a note, we are using non-free CUDA as to leverage high-performance preexisting code. CUDA, and non-free software in general, denies users the ability to study and modify it. This is detrimental to user freedom and to proper scientific review and experimentation. As such, we ask that you not share these modules widely as to encourage more open alternatives. To utilize any of our packaged software, you must add our channel to your channels.scm ( cons ( channel ( name 'guix-grex ) ( url \"https://github.com/GReX-Telescope/guix-grex.git\" ) ( branch \"main\" )) %default-channels ) Installation To start from a bare server, we need a few prerequisites. First, to actually build the install image, you need guix the guix binary installed. Installer isos will be provided, eventually. Clone the GReX Guix repo and run, this may take a while. $ guix system image --image-type = iso9660 grex/system/install.scm After that, make an installation media, either CD or USB and boot into it. If you want to make a USB, simply $ sudo dd if = <the iso that was created> of = /dev/<your USB> status = progress bs = 32M && sync Once you boot into the installation media, select \"Install using the shell based process\" Partitions We'll partition the servers with UEFI, you can use any tool you like for this, I like cfdisk . You'll need to make a 512M vfat partition for EFI and then ext4 the rest. Then, format and mount the partitions mkfs.ext4 /dev/root_partition mkfs.fat -F 32 /dev/efi_system_partition mount /dev/root_partition /mnt mkdir -p /mnt/boot/efi mount /dev/efi_system_partition /mnt/boot/efi Now we can setup the installation environment with herd start cow-store /mnt Initial Installation First, we need to grab the system configuration for this machine (assuming it exists). GReX servers we control will have their own configuration file. git clone https://github.com/GReX-Telescope/guix-grex First, we'll copy over the channels and update (this may take a while) mkdir -p ~/.config/guix Then open your editor of choice (we include vim and emacs in the installer image) and add the channels form from above to ~/.config/guix/channels.scm Then guix pull hash guix # This is necessary to ensure the updated profile path is active! Then initialize the system with cd guix-grex guix system -L . init grex/system/<specific server>.scm /mnt For example, we can provision the grex-01 server with guix system -L . init grex/system/grex-01 . scm /mnt Initial System Setup Now you can reboot and setup the users! First, we need to change the password. Login as root and passwd # For root passwd grex # For the GReX user Then, do the same steps of adding the GReX channels list to ~/.config/guix/channels.scm and then one final pull guix pull We're ready to go!","title":"Guix"},{"location":"software/guix/#pipeline-modules-and-guix","text":"Guix is a functional package manager and tool to instantiate and manage Unix-like operating systems. By functional, Guix defines packages through a purely functional deployment model in which every build is deterministic and is a pure function of the package's \"inputs\" or dependencies. This solves the problem of dependency hell and reproducability. For GReX, many of our software modules and components exist as forks of preexisting software as well as some custom code. To ensure all of these components work together in harmony, we'll host a guix channel that provides the build recipes for our software here . Most of this software, however, relies on the non-free CUDA runtime. Note As a note, we are using non-free CUDA as to leverage high-performance preexisting code. CUDA, and non-free software in general, denies users the ability to study and modify it. This is detrimental to user freedom and to proper scientific review and experimentation. As such, we ask that you not share these modules widely as to encourage more open alternatives. To utilize any of our packaged software, you must add our channel to your channels.scm ( cons ( channel ( name 'guix-grex ) ( url \"https://github.com/GReX-Telescope/guix-grex.git\" ) ( branch \"main\" )) %default-channels )","title":"Pipeline Modules and Guix"},{"location":"software/guix/#installation","text":"To start from a bare server, we need a few prerequisites. First, to actually build the install image, you need guix the guix binary installed. Installer isos will be provided, eventually. Clone the GReX Guix repo and run, this may take a while. $ guix system image --image-type = iso9660 grex/system/install.scm After that, make an installation media, either CD or USB and boot into it. If you want to make a USB, simply $ sudo dd if = <the iso that was created> of = /dev/<your USB> status = progress bs = 32M && sync Once you boot into the installation media, select \"Install using the shell based process\"","title":"Installation"},{"location":"software/guix/#partitions","text":"We'll partition the servers with UEFI, you can use any tool you like for this, I like cfdisk . You'll need to make a 512M vfat partition for EFI and then ext4 the rest. Then, format and mount the partitions mkfs.ext4 /dev/root_partition mkfs.fat -F 32 /dev/efi_system_partition mount /dev/root_partition /mnt mkdir -p /mnt/boot/efi mount /dev/efi_system_partition /mnt/boot/efi Now we can setup the installation environment with herd start cow-store /mnt","title":"Partitions"},{"location":"software/guix/#initial-installation","text":"First, we need to grab the system configuration for this machine (assuming it exists). GReX servers we control will have their own configuration file. git clone https://github.com/GReX-Telescope/guix-grex First, we'll copy over the channels and update (this may take a while) mkdir -p ~/.config/guix Then open your editor of choice (we include vim and emacs in the installer image) and add the channels form from above to ~/.config/guix/channels.scm Then guix pull hash guix # This is necessary to ensure the updated profile path is active! Then initialize the system with cd guix-grex guix system -L . init grex/system/<specific server>.scm /mnt For example, we can provision the grex-01 server with guix system -L . init grex/system/grex-01 . scm /mnt","title":"Initial Installation"},{"location":"software/guix/#initial-system-setup","text":"Now you can reboot and setup the users! First, we need to change the password. Login as root and passwd # For root passwd grex # For the GReX user Then, do the same steps of adding the GReX channels list to ~/.config/guix/channels.scm and then one final pull guix pull We're ready to go!","title":"Initial System Setup"},{"location":"software/overview/","text":"Software Overview There are two primary components to the software stack in GReX. First, the SNAP board must be configured and setup to send voltage data to the server. After that, the pipeline software should take care of the rest. This pipeline will exist as a composition of Guix packages and potentially as a Guix System definition for the entire system. This is to ensure determinisim in builds and to reduce the potential of mis-configuration. Pipeline Overview flowchart TD A[SNAP] -->|UDP| B[Byte Slurper] B -->|PSRDADA| C[RFI Cleaning] C -->|PSRDADA| D[Heimdall] D -->|Sockets| E[T2] Software Manifesto To limit downtime and maximize reproducability, we will try to adopt a consistent software development strategy. Primarily: Builds will be deterministic and reproducible Code will be version controlled, organized, and public Language Specific Rust No clippy warnings Avoid unsafe Document everything Formatted with rustfmt C++ Code will be formatted with clang-format's LLVM style Try to minimize (solve) all errors from -Wall Python Code will be formatted with Black Docstrings will follow the numpy format Gradual typing will be used (PEP 438) and checked with mypy or equivalent Environments will have pinned dependencies (reproducible)","title":"Overview"},{"location":"software/overview/#software-overview","text":"There are two primary components to the software stack in GReX. First, the SNAP board must be configured and setup to send voltage data to the server. After that, the pipeline software should take care of the rest. This pipeline will exist as a composition of Guix packages and potentially as a Guix System definition for the entire system. This is to ensure determinisim in builds and to reduce the potential of mis-configuration.","title":"Software Overview"},{"location":"software/overview/#pipeline-overview","text":"flowchart TD A[SNAP] -->|UDP| B[Byte Slurper] B -->|PSRDADA| C[RFI Cleaning] C -->|PSRDADA| D[Heimdall] D -->|Sockets| E[T2]","title":"Pipeline Overview"},{"location":"software/overview/#software-manifesto","text":"To limit downtime and maximize reproducability, we will try to adopt a consistent software development strategy. Primarily: Builds will be deterministic and reproducible Code will be version controlled, organized, and public","title":"Software Manifesto"},{"location":"software/overview/#language-specific","text":"","title":"Language Specific"},{"location":"software/overview/#rust","text":"No clippy warnings Avoid unsafe Document everything Formatted with rustfmt","title":"Rust"},{"location":"software/overview/#c","text":"Code will be formatted with clang-format's LLVM style Try to minimize (solve) all errors from -Wall","title":"C++"},{"location":"software/overview/#python","text":"Code will be formatted with Black Docstrings will follow the numpy format Gradual typing will be used (PEP 438) and checked with mypy or equivalent Environments will have pinned dependencies (reproducible)","title":"Python"},{"location":"software/pipeline/","text":"Pipeline","title":"Pipeline"},{"location":"software/pipeline/#pipeline","text":"","title":"Pipeline"},{"location":"software/server_setup/","text":"Server Setup Once you get your server (either from Puget systems or otherwise), we need to setup additional hardware, adjust some system settings, setup networking, and install the pipeline software. Hardware Setup The server straight from Puget does not have the GPU or 10 GbE network card installed, we will do this first. Open the case and remove the GPU retention bracket Remove the test GPU (T1000), keep this for later in case we need an aditional video output Install the 3090 Ti in the first GPU slot, this will take up three slots of space Install the network card in the bottom slot Wire the 3090 Ti power cable to the harness provided by Puget (they knew this was the GPU we were going to install) Remove the GPU retention clips from the retention bracket that would interfere with the card. It's too tall for it anyway. Replace the retention bracket and close the case. FIXME: Image Finally, we need to hook up a monitor to the 3090 Ti so we can setup the software Initial OS Setup On boot, you will be presented with the Ubuntu graphical interface. If a password is requested, the deafult is provided in the information booklet that came with the hardware. First, we will change the system password. Set this to something memorable. passwd $( whoami ) Now, we will update the system. There shouldn't be many updates if this is a new machine. sudo apt-get update sudo apt-get upgrade -y We will be editing a bunch of files, if you are comfy in the command line, you probably want to install some editors. Otherwise the graphical gedit tool will be fine. sudo apt-get install emacs vim -y Finally, we will set the hostname. We'll be using the grex-<affiliation>-<location> paradigm (just for clarity, no real reason not to). As in, the first server that is managed by Caltech at OVRO will be grex-caltech-ovro . sudo hostnamectl set-hostname <your-hostname> Some updates may require a reboot. If it asks, do that now. Graphics Drivers Puget ships with the NVIDIA drivers preinstalled (as it came with the T1000 card), but the version of CUDA may be out of date (as it was for us). To fix this, we're going to purge the system of NVIDIA stuff, add the official upstream NVIDIA package source, and build from there. First, let's get rid of all the existing NVIDIA stuff. sudo apt-get purge nvidia* sudo apt remove nvidia-* sudo rm /etc/apt/sources.list.d/cuda* sudo apt-get autoremove && sudo apt-get autoclean sudo rm -rf /usr/local/cuda* Now we'll do an update sudo apt update sudo apt upgrade And then install the NVIDIA repository sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update Next, we'll install the NVIDIA driver (515, unless anyone wants to update) sudo apt install libnvidia-common-515 sudo apt install libnvidia-gl-515 sudo apt install nvidia-driver-515 CUDA gets a little tricky, as we have to source the repo a bit more manually. wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600 sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub sudo add-apt-repository \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /\" sudo apt-get update sudo apt install cuda-11-7 Finally, we will fixup our paths for linking when we build stuff with CUDA. echo 'export PATH=/usr/local/cuda-11.7/bin:$PATH' >> ~/.bashrc echo 'export LD_LIBRARY_PATH=/usr/local/cuda-11.7/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc source ~/.bashrc sudo ldconfig Networking Now, we need to setup the networking for the GReX system. We will operate under the assumption that the internet-facing connection will get an IP address from a DHCP server. If that is not the case, consult whoever runs your network on the appropriate setup. Regardless of the WAN connection, the 10 GbE fiber connection to the GReX terminal will be configured the same. Overview The 10 GbE fiber port serves a few purposes. It is the main data transfer link between the FPGA in the field and the server, but it also carries the monitor and control for the box. This monitor and control connection includes the SNAP control connection and the Raspberry Pi. Both of these devices require an external DHCP server, which we have to provide on this port. Additionally, the 10 GbE switch in the box has its own default subnet for configuration ( 192.168.88.X ). To make everything talk to each other, we need to add two IPs on this port: one in the subnet of the switch's config interface, and the other for DHCP of the various devices. Netplan In /etc/netplan remove any files that are currently there. Then, create a new file called config.yaml with the following contents network : version : 2 renderer : networkd ethernets : # Two WAN interfaces. Configure this according to your network setup enp36s0f0 : dhcp4 : true enp36s0f1 : dhcp4 : true # 10 GbE connection over fiber to the box enp1s0f0 : mtu : 9000 addresses : - 192.168.0.1/24 - 192.168.88.2/24 Then apply with sudo netplan apply DHCP Server Now, we need to setup the DHCP server on the 10 GbE port. First, we install the DHCP server software: Setup the repository that includes kea curl -1sLf \\ 'https://dl.cloudsmith.io/public/isc/kea-2-3/setup.deb.sh' \\ | sudo -E bash Then install with sudo apt-get install isc-kea-dhcp4 Now, remove the example file: sudo rm /etc/kea/kea-dhcp4.conf And replace it with a file called kea-dhcp4.conf with the following contents { \"Dhcp4\" : { \"interfaces-config\" : { \"interfaces\" : [ \"enp1s0f0\" ], \"service-sockets-max-retries\" : 10 , \"service-sockets-retry-wait-time\" : 5000 }, \"lease-database\" : { \"type\" : \"memfile\" , \"persist\" : true , \"name\" : \"/var/lib/kea/kea-leases4.csv\" , \"lfc-interval\" : 3600 }, \"renew-timer\" : 15840 , \"rebind-timer\" : 27720 , \"valid-lifetime\" : 31680 , \"subnet4\" : [ { \"subnet\" : \"192.168.0.0/24\" , \"pools\" : [ { \"pool\" : \"192.168.0.2 - 192.168.0.100\" } ], \"option-data\" : [ { \"name\" : \"routers\" , \"data\" : \"192.168.0.1\" } ] } ] } } Finally enable the DHCP server service sudo systemctl enable isc-kea-dhcp4-server And check the status to make sure everything came up ok sudo systemctl status isc-kea-dhcp4-server Finally, sudo reboot After reboot, assuming your fiber line is plugged into the box, run cat /var/lib/kea/kea-leases4.csv You should see the raspberry pi. Advanced 10 GbE Settings Unfortunatley, the OS's default configuration for the 10 GbE network card is not optimized for our use-case of streaming time domain science data. As such, we need to adjust a few things. Add the following to /etc/sysctl.conf kernel.shmmax = 68719476736 kernel.shmall = 4294967296 net.core.rmem_max = 536870912 net.core.wmem_max = 536870912 net.core.netdev_max_backlog = 416384 net.core.optmem_max = 16777216 net.ipv4.udp_mem = 11416320 15221760 22832640 net.core.netdev_budget = 1024 net.ipv4.tcp_timestamps = 0 net.ipv4.tcp_sack = 0 net.ipv4.tcp_low_latency = 1 vm.swappiness=1 Then apply these changes with sudo sysctl --system Now, we need a program called ethtool to apply some more settings sudo apt-get install ethtool -y Now we will create a file to run on boot to apply a sequence of ethtool settings. Create the file /etc/rc.local with the following contents: #!/bin/env bash ethtool -G enp1s0f0 rx 4096 tx 4096 ethtool -A enp1s0f0 rx on ethtool -A enp1s0f0 tx on Make this file executable with sudo chmod +x /etc/rc.local Then enable the rc-local service sudo systemctl enable rc-local Now create the file /etc/systemd/system/rc-local.service with the following contents: [Unit] Description = /etc/rc.local Compatibility ConditionPathExists = /etc/rc.local [Service] Type = forking ExecStart = /etc/rc.local start TimeoutSec = 0 StandardOutput = tty RemainAfterExit = yes SysVStartPriority = 99 [Install] WantedBy = multi-user.target Finally, reboot Guix We use the deterministic pacakge manager Guix to deal with the more tricky to install things, so we never have to worry about stuff not building. You can install this with the Ubuntu package manager sudo apt-get install guix Then, we will add the repository of our pacakges by creating the following file ~/.config/guix/channels.scm ( cons ( channel ( name 'guix-grex ) ( url \"https://github.com/GReX-Telescope/guix-grex.git\" ) ( branch \"main\" )) %default-channels ) And then have it self-update with guix pull This step may take a while. Create (or add to) ~/.bash_profile GUIX_PROFILE = \" $HOME /.guix-profile\" . \" $GUIX_PROFILE /etc/profile\" if [ -f ~/.bashrc ] ; then source ~/.bashrc fi In here, we're also souring ~/.bashrc so we get it over ssh. Finally, install the pipeline dependencies with: guix install psrdada snap_bringup heimdall-astro Rust Many parts of the pipeline software are written in the Rust programming language. We will build be building this software from scratch, so we need to install the rust compiler and it's tooling. This is easy enough with rustup We need curl for the rustup installer, so sudo apt-get install curl -y Then run the installer, using all the default settings curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh Python To get out of version hell for python stuff not packaged with guix, we're using Poetry . To install it, we will: curl -sSL https://install.python-poetry.org | python3 - We need to make a few adjustments to ~/.bashrc to correct the paths and fix a bug. Append the following to the end. export PATH = \"/home/user/.local/bin: $PATH \" # Fix the \"Poetry: Failed to unlock the collection\" issue export PYTHON_KEYRING_BACKEND = keyring.backends.null.Keyring Go ahead and source ~/.bashrc now to get these changes in your shell. Pipeline Software To organize all the software needed for running the whole pipeline, make a new directory wherever you want (say, the home directory?) called grex mkdir $HOME /grex Then, move to this directory and clone the software cd $HOME /grex git clone https://github.com/GReX-Telescope/Pipeline pipeline git clone https://github.com/GReX-Telescope/GReX-T0 t0 git clone https://github.com/GReX-Telescope/GReX-T2 t2 T0 As T0 is bare rust source code, we need to compile it. This links against a few required binaries, so we'll install those: sudo apt-get install libhdf5-dev clang parallel -y cd into the t0 folder we just created and run: cargo build --release T2 T2 is a python project, built with poetry. To set it up we will cd into the t2 folder and run: poetry install Prometheus To save data about the server (CPU usage, RAM usage, etc) and to collect monitoring metrics from various pieces of pipeline software, we use the prometheus time series database. Each server will host its own database and push updates to the monitoring frontend Grafana. First, create a new group and user sudo groupadd --system prometheus sudo useradd -s /sbin/nologin --system -g prometheus prometheus Next, we create the path for the database. Puget setup the system so the large storage drive is mounted at /hdd . We will keep the database there. sudo mkdir /hdd/prometheus Prometheus primary configuration files directory is /etc/prometheus/. It will have some sub-directories: for i in rules rules.d files_sd ; do sudo mkdir -p /etc/prometheus/ ${ i } ; done Next, get a copy of the prometheus binaries, extract, and move into that dir mkdir -p /tmp/prometheus && cd /tmp/prometheus curl -s https://api.github.com/repos/prometheus/prometheus/releases/latest | grep browser_download_url | grep linux-amd64 | cut -d '\"' -f 4 | wget -qi - tar xvf prometheus*.tar.gz cd prometheus*/ Install the files by moving to /usr/local/bin sudo mv prometheus promtool /usr/local/bin/ Move Prometheus configuration files to etc sudo mv prometheus.yml /etc/prometheus/prometheus.yml sudo mv consoles/ console_libraries/ /etc/prometheus/ Now, we configure. Open up /etc/prometheus/prometheus.yml and edit to contain: global: scrape_interval: 10s evaluation_interval: 10s scrape_configs: - job_name: \"prometheus\" static_configs: - targets: [\"localhost:9090\", \"localhost:9100\", \"localhost:8083\"] remote_write: - url: <grafana-url> basic_auth: username: <grafana username> password: <grafana api key> If you are hooking up to our grafana instance, you will get an API key from the project, otherwise you'd create a remote_write section that reflects your monitoring stack. Now, create a systemd unit to run the database in the file /etc/systemd/system/prometheus.service [Unit] Description = Prometheus Documentation = https://prometheus.io/docs/introduction/overview/ Wants = network-online.target After = network-online.target [Service] Type = simple User = prometheus Group = prometheus ExecReload = /bin/kill -HUP \\$MAINPID ExecStart = /usr/local/bin/prometheus \\ --config.file = /etc/prometheus/prometheus.yml \\ --storage.tsdb.path = /hdd/prometheus \\ --web.console.templates = /etc/prometheus/consoles \\ --web.console.libraries = /etc/prometheus/console_libraries \\ --web.listen-address = 0.0.0.0:9090 \\ --web.external-url = SyslogIdentifier = prometheus Restart = always [Install] WantedBy = multi-user.target Now update all the permissions of the things we've mucked with to make sure prometheus can use them for i in rules rules.d files_sd ; do sudo chown -R prometheus:prometheus /etc/prometheus/ ${ i } ; done for i in rules rules.d files_sd ; do sudo chmod -R 775 /etc/prometheus/ ${ i } ; done sudo chown -R prometheus:prometheus /hdd/prometheus/ Finally, reload systemd and start the service sudo systemctl daemon-reload sudo systemctl start prometheus sudo systemctl enable prometheus Now we will install the node-exporter, which gives us metrics of the computer itself. sudo apt-get install prometheus-node-exporter Turning on the SNAP SSH into the Pi (password is raspberry) via ssh pi@<the ip from dhcp-lease-list> Then on the pi, run python3 pwup_snap.py There also exists pwdn_snap.py , with an obvious purpose. Preconfigured These steps should already be performed before we ship a box, but for completeness, here are the steps. Valon We need to configure the valon synthesizer to act as the LO for the downconverter and the reference clock for the SNAP ADC. Use the GUI tool here to load this configuration file. Next, go to synthesizer -> write registers. Then, save the configuration to flash to preserve this configuration across reboots. Switch With the box connected and powered on, create an SSH relay to the switch's configuration interface with ssh -L 8291 :192.168.88.1:8291 user@<the ip address of the server> Then, using winbox connect to localhost, select files on the left, and upload this config file . This should trigger a reboot. SNAP Golden Image In order to use the SNAP's flash, you must first set switch S1 so that switches 2 and 5 are set to on. (In the on position, the switches are moved towards the edge of the PCB). The other switches on S1 should be off. The SNAP needs to be programmed with a \"golden image\" that runs on boot, acting as a bootstrapping device. To flash this, you need the free Vivado Lab Edition and the expensive Xilinx Platform Cable. Under tools go to Add configuration memory device . Entire configuration n25q256-3.3v-spi-x1_x2_x4 Unplug programmer before rebooting.","title":"Server Setup"},{"location":"software/server_setup/#server-setup","text":"Once you get your server (either from Puget systems or otherwise), we need to setup additional hardware, adjust some system settings, setup networking, and install the pipeline software.","title":"Server Setup"},{"location":"software/server_setup/#hardware-setup","text":"The server straight from Puget does not have the GPU or 10 GbE network card installed, we will do this first. Open the case and remove the GPU retention bracket Remove the test GPU (T1000), keep this for later in case we need an aditional video output Install the 3090 Ti in the first GPU slot, this will take up three slots of space Install the network card in the bottom slot Wire the 3090 Ti power cable to the harness provided by Puget (they knew this was the GPU we were going to install) Remove the GPU retention clips from the retention bracket that would interfere with the card. It's too tall for it anyway. Replace the retention bracket and close the case. FIXME: Image Finally, we need to hook up a monitor to the 3090 Ti so we can setup the software","title":"Hardware Setup"},{"location":"software/server_setup/#initial-os-setup","text":"On boot, you will be presented with the Ubuntu graphical interface. If a password is requested, the deafult is provided in the information booklet that came with the hardware. First, we will change the system password. Set this to something memorable. passwd $( whoami ) Now, we will update the system. There shouldn't be many updates if this is a new machine. sudo apt-get update sudo apt-get upgrade -y We will be editing a bunch of files, if you are comfy in the command line, you probably want to install some editors. Otherwise the graphical gedit tool will be fine. sudo apt-get install emacs vim -y Finally, we will set the hostname. We'll be using the grex-<affiliation>-<location> paradigm (just for clarity, no real reason not to). As in, the first server that is managed by Caltech at OVRO will be grex-caltech-ovro . sudo hostnamectl set-hostname <your-hostname> Some updates may require a reboot. If it asks, do that now.","title":"Initial OS Setup"},{"location":"software/server_setup/#graphics-drivers","text":"Puget ships with the NVIDIA drivers preinstalled (as it came with the T1000 card), but the version of CUDA may be out of date (as it was for us). To fix this, we're going to purge the system of NVIDIA stuff, add the official upstream NVIDIA package source, and build from there. First, let's get rid of all the existing NVIDIA stuff. sudo apt-get purge nvidia* sudo apt remove nvidia-* sudo rm /etc/apt/sources.list.d/cuda* sudo apt-get autoremove && sudo apt-get autoclean sudo rm -rf /usr/local/cuda* Now we'll do an update sudo apt update sudo apt upgrade And then install the NVIDIA repository sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update Next, we'll install the NVIDIA driver (515, unless anyone wants to update) sudo apt install libnvidia-common-515 sudo apt install libnvidia-gl-515 sudo apt install nvidia-driver-515 CUDA gets a little tricky, as we have to source the repo a bit more manually. wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600 sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub sudo add-apt-repository \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /\" sudo apt-get update sudo apt install cuda-11-7 Finally, we will fixup our paths for linking when we build stuff with CUDA. echo 'export PATH=/usr/local/cuda-11.7/bin:$PATH' >> ~/.bashrc echo 'export LD_LIBRARY_PATH=/usr/local/cuda-11.7/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc source ~/.bashrc sudo ldconfig","title":"Graphics Drivers"},{"location":"software/server_setup/#networking","text":"Now, we need to setup the networking for the GReX system. We will operate under the assumption that the internet-facing connection will get an IP address from a DHCP server. If that is not the case, consult whoever runs your network on the appropriate setup. Regardless of the WAN connection, the 10 GbE fiber connection to the GReX terminal will be configured the same.","title":"Networking"},{"location":"software/server_setup/#overview","text":"The 10 GbE fiber port serves a few purposes. It is the main data transfer link between the FPGA in the field and the server, but it also carries the monitor and control for the box. This monitor and control connection includes the SNAP control connection and the Raspberry Pi. Both of these devices require an external DHCP server, which we have to provide on this port. Additionally, the 10 GbE switch in the box has its own default subnet for configuration ( 192.168.88.X ). To make everything talk to each other, we need to add two IPs on this port: one in the subnet of the switch's config interface, and the other for DHCP of the various devices.","title":"Overview"},{"location":"software/server_setup/#netplan","text":"In /etc/netplan remove any files that are currently there. Then, create a new file called config.yaml with the following contents network : version : 2 renderer : networkd ethernets : # Two WAN interfaces. Configure this according to your network setup enp36s0f0 : dhcp4 : true enp36s0f1 : dhcp4 : true # 10 GbE connection over fiber to the box enp1s0f0 : mtu : 9000 addresses : - 192.168.0.1/24 - 192.168.88.2/24 Then apply with sudo netplan apply","title":"Netplan"},{"location":"software/server_setup/#dhcp-server","text":"Now, we need to setup the DHCP server on the 10 GbE port. First, we install the DHCP server software: Setup the repository that includes kea curl -1sLf \\ 'https://dl.cloudsmith.io/public/isc/kea-2-3/setup.deb.sh' \\ | sudo -E bash Then install with sudo apt-get install isc-kea-dhcp4 Now, remove the example file: sudo rm /etc/kea/kea-dhcp4.conf And replace it with a file called kea-dhcp4.conf with the following contents { \"Dhcp4\" : { \"interfaces-config\" : { \"interfaces\" : [ \"enp1s0f0\" ], \"service-sockets-max-retries\" : 10 , \"service-sockets-retry-wait-time\" : 5000 }, \"lease-database\" : { \"type\" : \"memfile\" , \"persist\" : true , \"name\" : \"/var/lib/kea/kea-leases4.csv\" , \"lfc-interval\" : 3600 }, \"renew-timer\" : 15840 , \"rebind-timer\" : 27720 , \"valid-lifetime\" : 31680 , \"subnet4\" : [ { \"subnet\" : \"192.168.0.0/24\" , \"pools\" : [ { \"pool\" : \"192.168.0.2 - 192.168.0.100\" } ], \"option-data\" : [ { \"name\" : \"routers\" , \"data\" : \"192.168.0.1\" } ] } ] } } Finally enable the DHCP server service sudo systemctl enable isc-kea-dhcp4-server And check the status to make sure everything came up ok sudo systemctl status isc-kea-dhcp4-server Finally, sudo reboot After reboot, assuming your fiber line is plugged into the box, run cat /var/lib/kea/kea-leases4.csv You should see the raspberry pi.","title":"DHCP Server"},{"location":"software/server_setup/#advanced-10-gbe-settings","text":"Unfortunatley, the OS's default configuration for the 10 GbE network card is not optimized for our use-case of streaming time domain science data. As such, we need to adjust a few things. Add the following to /etc/sysctl.conf kernel.shmmax = 68719476736 kernel.shmall = 4294967296 net.core.rmem_max = 536870912 net.core.wmem_max = 536870912 net.core.netdev_max_backlog = 416384 net.core.optmem_max = 16777216 net.ipv4.udp_mem = 11416320 15221760 22832640 net.core.netdev_budget = 1024 net.ipv4.tcp_timestamps = 0 net.ipv4.tcp_sack = 0 net.ipv4.tcp_low_latency = 1 vm.swappiness=1 Then apply these changes with sudo sysctl --system Now, we need a program called ethtool to apply some more settings sudo apt-get install ethtool -y Now we will create a file to run on boot to apply a sequence of ethtool settings. Create the file /etc/rc.local with the following contents: #!/bin/env bash ethtool -G enp1s0f0 rx 4096 tx 4096 ethtool -A enp1s0f0 rx on ethtool -A enp1s0f0 tx on Make this file executable with sudo chmod +x /etc/rc.local Then enable the rc-local service sudo systemctl enable rc-local Now create the file /etc/systemd/system/rc-local.service with the following contents: [Unit] Description = /etc/rc.local Compatibility ConditionPathExists = /etc/rc.local [Service] Type = forking ExecStart = /etc/rc.local start TimeoutSec = 0 StandardOutput = tty RemainAfterExit = yes SysVStartPriority = 99 [Install] WantedBy = multi-user.target Finally, reboot","title":"Advanced 10 GbE Settings"},{"location":"software/server_setup/#guix","text":"We use the deterministic pacakge manager Guix to deal with the more tricky to install things, so we never have to worry about stuff not building. You can install this with the Ubuntu package manager sudo apt-get install guix Then, we will add the repository of our pacakges by creating the following file ~/.config/guix/channels.scm ( cons ( channel ( name 'guix-grex ) ( url \"https://github.com/GReX-Telescope/guix-grex.git\" ) ( branch \"main\" )) %default-channels ) And then have it self-update with guix pull This step may take a while. Create (or add to) ~/.bash_profile GUIX_PROFILE = \" $HOME /.guix-profile\" . \" $GUIX_PROFILE /etc/profile\" if [ -f ~/.bashrc ] ; then source ~/.bashrc fi In here, we're also souring ~/.bashrc so we get it over ssh. Finally, install the pipeline dependencies with: guix install psrdada snap_bringup heimdall-astro","title":"Guix"},{"location":"software/server_setup/#rust","text":"Many parts of the pipeline software are written in the Rust programming language. We will build be building this software from scratch, so we need to install the rust compiler and it's tooling. This is easy enough with rustup We need curl for the rustup installer, so sudo apt-get install curl -y Then run the installer, using all the default settings curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh","title":"Rust"},{"location":"software/server_setup/#python","text":"To get out of version hell for python stuff not packaged with guix, we're using Poetry . To install it, we will: curl -sSL https://install.python-poetry.org | python3 - We need to make a few adjustments to ~/.bashrc to correct the paths and fix a bug. Append the following to the end. export PATH = \"/home/user/.local/bin: $PATH \" # Fix the \"Poetry: Failed to unlock the collection\" issue export PYTHON_KEYRING_BACKEND = keyring.backends.null.Keyring Go ahead and source ~/.bashrc now to get these changes in your shell.","title":"Python"},{"location":"software/server_setup/#pipeline-software","text":"To organize all the software needed for running the whole pipeline, make a new directory wherever you want (say, the home directory?) called grex mkdir $HOME /grex Then, move to this directory and clone the software cd $HOME /grex git clone https://github.com/GReX-Telescope/Pipeline pipeline git clone https://github.com/GReX-Telescope/GReX-T0 t0 git clone https://github.com/GReX-Telescope/GReX-T2 t2","title":"Pipeline Software"},{"location":"software/server_setup/#t0","text":"As T0 is bare rust source code, we need to compile it. This links against a few required binaries, so we'll install those: sudo apt-get install libhdf5-dev clang parallel -y cd into the t0 folder we just created and run: cargo build --release","title":"T0"},{"location":"software/server_setup/#t2","text":"T2 is a python project, built with poetry. To set it up we will cd into the t2 folder and run: poetry install","title":"T2"},{"location":"software/server_setup/#prometheus","text":"To save data about the server (CPU usage, RAM usage, etc) and to collect monitoring metrics from various pieces of pipeline software, we use the prometheus time series database. Each server will host its own database and push updates to the monitoring frontend Grafana. First, create a new group and user sudo groupadd --system prometheus sudo useradd -s /sbin/nologin --system -g prometheus prometheus Next, we create the path for the database. Puget setup the system so the large storage drive is mounted at /hdd . We will keep the database there. sudo mkdir /hdd/prometheus Prometheus primary configuration files directory is /etc/prometheus/. It will have some sub-directories: for i in rules rules.d files_sd ; do sudo mkdir -p /etc/prometheus/ ${ i } ; done Next, get a copy of the prometheus binaries, extract, and move into that dir mkdir -p /tmp/prometheus && cd /tmp/prometheus curl -s https://api.github.com/repos/prometheus/prometheus/releases/latest | grep browser_download_url | grep linux-amd64 | cut -d '\"' -f 4 | wget -qi - tar xvf prometheus*.tar.gz cd prometheus*/ Install the files by moving to /usr/local/bin sudo mv prometheus promtool /usr/local/bin/ Move Prometheus configuration files to etc sudo mv prometheus.yml /etc/prometheus/prometheus.yml sudo mv consoles/ console_libraries/ /etc/prometheus/ Now, we configure. Open up /etc/prometheus/prometheus.yml and edit to contain: global: scrape_interval: 10s evaluation_interval: 10s scrape_configs: - job_name: \"prometheus\" static_configs: - targets: [\"localhost:9090\", \"localhost:9100\", \"localhost:8083\"] remote_write: - url: <grafana-url> basic_auth: username: <grafana username> password: <grafana api key> If you are hooking up to our grafana instance, you will get an API key from the project, otherwise you'd create a remote_write section that reflects your monitoring stack. Now, create a systemd unit to run the database in the file /etc/systemd/system/prometheus.service [Unit] Description = Prometheus Documentation = https://prometheus.io/docs/introduction/overview/ Wants = network-online.target After = network-online.target [Service] Type = simple User = prometheus Group = prometheus ExecReload = /bin/kill -HUP \\$MAINPID ExecStart = /usr/local/bin/prometheus \\ --config.file = /etc/prometheus/prometheus.yml \\ --storage.tsdb.path = /hdd/prometheus \\ --web.console.templates = /etc/prometheus/consoles \\ --web.console.libraries = /etc/prometheus/console_libraries \\ --web.listen-address = 0.0.0.0:9090 \\ --web.external-url = SyslogIdentifier = prometheus Restart = always [Install] WantedBy = multi-user.target Now update all the permissions of the things we've mucked with to make sure prometheus can use them for i in rules rules.d files_sd ; do sudo chown -R prometheus:prometheus /etc/prometheus/ ${ i } ; done for i in rules rules.d files_sd ; do sudo chmod -R 775 /etc/prometheus/ ${ i } ; done sudo chown -R prometheus:prometheus /hdd/prometheus/ Finally, reload systemd and start the service sudo systemctl daemon-reload sudo systemctl start prometheus sudo systemctl enable prometheus Now we will install the node-exporter, which gives us metrics of the computer itself. sudo apt-get install prometheus-node-exporter","title":"Prometheus"},{"location":"software/server_setup/#turning-on-the-snap","text":"SSH into the Pi (password is raspberry) via ssh pi@<the ip from dhcp-lease-list> Then on the pi, run python3 pwup_snap.py There also exists pwdn_snap.py , with an obvious purpose.","title":"Turning on the SNAP"},{"location":"software/server_setup/#preconfigured","text":"These steps should already be performed before we ship a box, but for completeness, here are the steps.","title":"Preconfigured"},{"location":"software/server_setup/#valon","text":"We need to configure the valon synthesizer to act as the LO for the downconverter and the reference clock for the SNAP ADC. Use the GUI tool here to load this configuration file. Next, go to synthesizer -> write registers. Then, save the configuration to flash to preserve this configuration across reboots.","title":"Valon"},{"location":"software/server_setup/#switch","text":"With the box connected and powered on, create an SSH relay to the switch's configuration interface with ssh -L 8291 :192.168.88.1:8291 user@<the ip address of the server> Then, using winbox connect to localhost, select files on the left, and upload this config file . This should trigger a reboot.","title":"Switch"},{"location":"software/server_setup/#snap-golden-image","text":"In order to use the SNAP's flash, you must first set switch S1 so that switches 2 and 5 are set to on. (In the on position, the switches are moved towards the edge of the PCB). The other switches on S1 should be off. The SNAP needs to be programmed with a \"golden image\" that runs on boot, acting as a bootstrapping device. To flash this, you need the free Vivado Lab Edition and the expensive Xilinx Platform Cable. Under tools go to Add configuration memory device . Entire configuration n25q256-3.3v-spi-x1_x2_x4 Unplug programmer before rebooting.","title":"SNAP Golden Image"},{"location":"software/snap/","text":"SNAP Configuration and Bringup The digital backend to the GReX system is a SNAP board from the CASPER group at Berkeley. This board contains the analog to digital converters and Xilinx FPGA to perform the digitization and F-engine components of the system. The setup and configuration of this board has seemingly never been well documented, so we'll try to make it as painless as possible here. The FPGA Simulink model is stored here with the latest releases found here . Grab the latest fpg file, and you're good to go - no reason to recompile it. TAPCP and the Raspberry Pi The gateware we've built for the SNAP board includes a small CPU called the MicroBlaze that hosts a small webserver to deal with runtime interactions with FPGA memory. This server gets an IP address from a DHCP server running on the main capture computer. This interface can also be used to reprogram the SNAP if the gateware changes. By deafult, we'll ship SNAP boards that have the GReX gateware preprogramed, but it's always possible to reprogram it. This interface is over the UDP protocol TFTP, where the folks at CASPER have wrapped reading and writing files as the interaction to both the flash and FPU memory. We've written a wrapper to the so called \"TAPCP\" protocol here . It is with this library that the packet capture code informs the SNAP when to activate timing signals. FPGA Clocks, References, PPS, Synthesizers The FPGA needs an external clock source, which is being provided via one of the output of the Valon synthesizer in box. Additionally, the board needs \"pulse per second\" (PPS) ticks, which come from the GPS reciever. If the user is in a place which doesn't have GPS (maybe in a lab during testing), we can override that check. TODO! Somehow TODO! Check clock is good and everything is locked? Does this happen after ADC configuration?","title":"SNAP"},{"location":"software/snap/#snap-configuration-and-bringup","text":"The digital backend to the GReX system is a SNAP board from the CASPER group at Berkeley. This board contains the analog to digital converters and Xilinx FPGA to perform the digitization and F-engine components of the system. The setup and configuration of this board has seemingly never been well documented, so we'll try to make it as painless as possible here. The FPGA Simulink model is stored here with the latest releases found here . Grab the latest fpg file, and you're good to go - no reason to recompile it.","title":"SNAP Configuration and Bringup"},{"location":"software/snap/#tapcp-and-the-raspberry-pi","text":"The gateware we've built for the SNAP board includes a small CPU called the MicroBlaze that hosts a small webserver to deal with runtime interactions with FPGA memory. This server gets an IP address from a DHCP server running on the main capture computer. This interface can also be used to reprogram the SNAP if the gateware changes. By deafult, we'll ship SNAP boards that have the GReX gateware preprogramed, but it's always possible to reprogram it. This interface is over the UDP protocol TFTP, where the folks at CASPER have wrapped reading and writing files as the interaction to both the flash and FPU memory. We've written a wrapper to the so called \"TAPCP\" protocol here . It is with this library that the packet capture code informs the SNAP when to activate timing signals.","title":"TAPCP and the Raspberry Pi"},{"location":"software/snap/#fpga-clocks-references-pps-synthesizers","text":"The FPGA needs an external clock source, which is being provided via one of the output of the Valon synthesizer in box. Additionally, the board needs \"pulse per second\" (PPS) ticks, which come from the GPS reciever. If the user is in a place which doesn't have GPS (maybe in a lab during testing), we can override that check. TODO! Somehow TODO! Check clock is good and everything is locked? Does this happen after ADC configuration?","title":"FPGA Clocks, References, PPS, Synthesizers"},{"location":"software/stack/","text":"Software Stack There are several moving parts to this whole project, most of which are organized in our GitHub organization . The notable pieces of software are: snapctl - SNAP bringup and configuration byte_slurper - UDP Packet capture and exfil to heimdal heimdall - Our fork of the pulse detection pipeline which removes clustering and RFI excision T2, T3, etc fem_mnc - Monitor and control of the FEM through the Pi's network connection These are supported by some fundamental libraries sigproc_filterbank - A rust library for reading/writing SIGPROC filterbank files psrdada-rs - A rust library for interacting with PSRDADA buffers casperfpga_rs - A rust library for interacting with the SNAP board over TAPCP","title":"Software Stack"},{"location":"software/stack/#software-stack","text":"There are several moving parts to this whole project, most of which are organized in our GitHub organization . The notable pieces of software are: snapctl - SNAP bringup and configuration byte_slurper - UDP Packet capture and exfil to heimdal heimdall - Our fork of the pulse detection pipeline which removes clustering and RFI excision T2, T3, etc fem_mnc - Monitor and control of the FEM through the Pi's network connection These are supported by some fundamental libraries sigproc_filterbank - A rust library for reading/writing SIGPROC filterbank files psrdada-rs - A rust library for interacting with PSRDADA buffers casperfpga_rs - A rust library for interacting with the SNAP board over TAPCP","title":"Software Stack"}]}