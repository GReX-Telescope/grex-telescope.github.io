{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Galactic Radio Explorer Telescope Welcome to the documentation for the Galactic Radio Explorer Telescope! In the tabs above, you'll find the documentation for the hardware and software architectures as well as guides for construction and setup. These docs are a WIP and once we deploy, contributions are welcome!","title":"Home"},{"location":"#the-galactic-radio-explorer-telescope","text":"Welcome to the documentation for the Galactic Radio Explorer Telescope! In the tabs above, you'll find the documentation for the hardware and software architectures as well as guides for construction and setup. These docs are a WIP and once we deploy, contributions are welcome!","title":"The Galactic Radio Explorer Telescope"},{"location":"about/","text":"The Galactic Radio Telescope is an new low-cost radio telescope designed to be an all-sky monitor for bright radio bursts. Building on the success of STARE2, we will search for fast radio bursts (FRBs) emitted from Galactic magnetars as well as bursts from nearby galaxies. GReX will search down to ten microseconds time resolution, allowing us to find new super giant radio pulses from Milky Way pulsars and study their broadband emission. The proposed instrument will employ ultra-wide band (0.7-2 GHz) feeds coupled to a high performance (receiver temperature 10 K) low noise amplifier (LNA) originally developed for the DSA-110 and DSA-2000 projects. In GReX Phase I (GReX-I), unit systems will be deployed at Owens Valley Radio Observatory (OVRO) and Big Smoky Valley, Nevada. Phase II will expand the array, placing feeds in India, Australia, and elsewhere in order to build up to continuous coverage of nearly 4\u03c0 steradians and to increase our exposure to the Galactic plane. We model the local magnetar population to forecast for GReX, finding the improved sensitivity and increased exposure to the Galactic plane could lead to dozens of FRB-like bursts per year. For more information, read our paper here ! Team Project Scientist - Dr. Liam Connor, PhD Project Engineer - Kiran Shila, MSEE","title":"About"},{"location":"about/#team","text":"Project Scientist - Dr. Liam Connor, PhD Project Engineer - Kiran Shila, MSEE","title":"Team"},{"location":"hardware/assembly/","text":"Assembly Guide TODO! Once we have the complete pacakge, write a step by step guide for assembly and installation","title":"Assembly Guide"},{"location":"hardware/assembly/#assembly-guide","text":"TODO! Once we have the complete pacakge, write a step by step guide for assembly and installation","title":"Assembly Guide"},{"location":"hardware/box/","text":"The Box TODO! Once the box is done, document cables, routing, etc.","title":"The Box"},{"location":"hardware/box/#the-box","text":"TODO! Once the box is done, document cables, routing, etc.","title":"The Box"},{"location":"hardware/feed/","text":"Feed Antenna TODO! Jonas info","title":"Feed Antenna"},{"location":"hardware/feed/#feed-antenna","text":"TODO! Jonas info","title":"Feed Antenna"},{"location":"hardware/fem/","text":"Frontend Module The frontend module (FEM) is a device that performs the analog signal processing after the LNAs. This includes filtering, downconversion, and amplification. Additionally, this module provides rudimentary monitor and control support. Bare PCB Completed Module Hardware Design The hardware design itself is implemented in the free KiCAD program and is available here . To manufacture from gerber files, the stackup needs to be JLC7628 from JLCPCB. The current hardware uses ENIG to help reflow of the fine-pitch components. Schematics BOM Case Firmware Design The RF hardware mostly operates without the intervention of any software. The only step required to use the RF hardware is to set the valid attenuation level, which defaults to 0 dB. As such, the primary goal of the digital section of the FEM is to perform Monitor and Control (MnC). MnC is achieved via an 115200 baud 3.3V UART interface on the main connector. The firmware design is carried out in the Rust programming language, and whose source can be found here . Monitor Every 1 second on UART (115200 baud), JSON payload of monitor data is sent out with the following schema { \"boardTemp\" : 29.6 , \"voltages\" : { \"rawInput\" : 6.2 , \"analog\" : 4.9 , \"lnaOne\" : 5.3 , \"lnaTwo\" : 5.3 }, \"currents\" : { \"rawInput\" : 0.723 , \"analog\" : 0.53 , \"lnaOne\" : 0.053 , \"lnaTwo\" : 0.052 }, \"ifPower\" : { \"channelOne\" : -0.3 , \"channelTwo\" : -2.1 }, \"control\" : { \"calOne\" : false , \"calTwo\" : false , \"lnaOnePowered\" : true , \"lnaTwoPowered\" : true , \"attenuationLevel\" : 3 , \"ifPowerThreshold\" : -10 } } Control The control payload must be a complete JSON object of the following form: { \"control\" : { \"calOne\" : false , \"calTwo\" : false , \"lnaOnePowered\" : true , \"lnaTwoPowered\" : true , \"attenuationLevel\" : 3 , \"ifPowerThreshold\" : -10 } } Over UART, there is control for enabling/disabling the calibration output, the LNA bias, and the interstage IF attenuator. For the digital attenuator, there are four levels (0-3), representing 0, 4, 8, 12 dB. This attenuator is to maximize the dynamic range of the ADC and can be set for environmental RFI levels. Physical Interface There are eight LEDs on the front panel. Four red LEDs to indicate power statess, two blue LEDs for serial activity, and two green LEDs for system status. The green LEDs will be enabled when the IF power is at a nominal level and will flash when the calibration signal is enabled.","title":"Fronend Module"},{"location":"hardware/fem/#frontend-module","text":"The frontend module (FEM) is a device that performs the analog signal processing after the LNAs. This includes filtering, downconversion, and amplification. Additionally, this module provides rudimentary monitor and control support. Bare PCB Completed Module","title":"Frontend Module"},{"location":"hardware/fem/#hardware-design","text":"The hardware design itself is implemented in the free KiCAD program and is available here . To manufacture from gerber files, the stackup needs to be JLC7628 from JLCPCB. The current hardware uses ENIG to help reflow of the fine-pitch components. Schematics BOM Case","title":"Hardware Design"},{"location":"hardware/fem/#firmware-design","text":"The RF hardware mostly operates without the intervention of any software. The only step required to use the RF hardware is to set the valid attenuation level, which defaults to 0 dB. As such, the primary goal of the digital section of the FEM is to perform Monitor and Control (MnC). MnC is achieved via an 115200 baud 3.3V UART interface on the main connector. The firmware design is carried out in the Rust programming language, and whose source can be found here .","title":"Firmware Design"},{"location":"hardware/fem/#monitor","text":"Every 1 second on UART (115200 baud), JSON payload of monitor data is sent out with the following schema { \"boardTemp\" : 29.6 , \"voltages\" : { \"rawInput\" : 6.2 , \"analog\" : 4.9 , \"lnaOne\" : 5.3 , \"lnaTwo\" : 5.3 }, \"currents\" : { \"rawInput\" : 0.723 , \"analog\" : 0.53 , \"lnaOne\" : 0.053 , \"lnaTwo\" : 0.052 }, \"ifPower\" : { \"channelOne\" : -0.3 , \"channelTwo\" : -2.1 }, \"control\" : { \"calOne\" : false , \"calTwo\" : false , \"lnaOnePowered\" : true , \"lnaTwoPowered\" : true , \"attenuationLevel\" : 3 , \"ifPowerThreshold\" : -10 } }","title":"Monitor"},{"location":"hardware/fem/#control","text":"The control payload must be a complete JSON object of the following form: { \"control\" : { \"calOne\" : false , \"calTwo\" : false , \"lnaOnePowered\" : true , \"lnaTwoPowered\" : true , \"attenuationLevel\" : 3 , \"ifPowerThreshold\" : -10 } } Over UART, there is control for enabling/disabling the calibration output, the LNA bias, and the interstage IF attenuator. For the digital attenuator, there are four levels (0-3), representing 0, 4, 8, 12 dB. This attenuator is to maximize the dynamic range of the ADC and can be set for environmental RFI levels.","title":"Control"},{"location":"hardware/fem/#physical-interface","text":"There are eight LEDs on the front panel. Four red LEDs to indicate power statess, two blue LEDs for serial activity, and two green LEDs for system status. The green LEDs will be enabled when the IF power is at a nominal level and will flash when the calibration signal is enabled.","title":"Physical Interface"},{"location":"hardware/fpga/","text":"Digital Backend TODO! Describe hardware interfaces of SNAP.","title":"Digital Backend"},{"location":"hardware/fpga/#digital-backend","text":"TODO! Describe hardware interfaces of SNAP.","title":"Digital Backend"},{"location":"hardware/overview/","text":"Hardware Overview The GReX hardware system has several \"top level\" components, which constitute the entire system. These include the feed antenna and low noise amplifiers (LNA), the frontend module , the digital backend , and of course the server. The following diagrams lay out general overview of the interconnections. Showing them all at once would be a bit much, so they're broken down here into discrete kinds of signals. RF Signal Path flowchart TD A[Feed] B[FEM] C[SNAP] D[Server] A --> L1[LNA] A --> L2[LNA] L1 -->|H Pol| B L2 -->|V Pol| B subgraph The Box B -->|H Pol| C B -->|V Pol| C end C -->|10 GbE| D[Server] Power Distribution flowchart BT L1[LNA] L2[LNA] B[FEM] C[SNAP] S[Switching Supply] R[Linear Regulator] P[Raspberry Pi] M[Mains Power] V[Synthesizer] G[GPS Receiver] M -->|120-240V AC| S subgraph The Box R -->|6.5V DC| V P -->|\"5V DC (USB)\"| G S -->|12V DC| C S -->|12V DC| R R -->|6.5V DC| B C -->|5V DC| P end B --->|5.5V DC| L1 B --->|5.5V DC| L2 Clocks, References and Timing flowchart BT B[FEM] V[Synthesizer] G[GPS Receiver] S[SNAP] A[GPS Antenna] subgraph The Box G -->|10 MHz| V G -->|PPS| S V -->|500 MHz| S V -->|1030 MHz| B end A --> G Monitor and Control flowchart TB B[FEM] S[SNAP] P[Raspberry Pi] D[Server] subgraph The Box P <-->|UART| B S <-->|GPIO| P end P <--->|1 GbE| D","title":"Overview"},{"location":"hardware/overview/#hardware-overview","text":"The GReX hardware system has several \"top level\" components, which constitute the entire system. These include the feed antenna and low noise amplifiers (LNA), the frontend module , the digital backend , and of course the server. The following diagrams lay out general overview of the interconnections. Showing them all at once would be a bit much, so they're broken down here into discrete kinds of signals.","title":"Hardware Overview"},{"location":"hardware/overview/#rf-signal-path","text":"flowchart TD A[Feed] B[FEM] C[SNAP] D[Server] A --> L1[LNA] A --> L2[LNA] L1 -->|H Pol| B L2 -->|V Pol| B subgraph The Box B -->|H Pol| C B -->|V Pol| C end C -->|10 GbE| D[Server]","title":"RF Signal Path"},{"location":"hardware/overview/#power-distribution","text":"flowchart BT L1[LNA] L2[LNA] B[FEM] C[SNAP] S[Switching Supply] R[Linear Regulator] P[Raspberry Pi] M[Mains Power] V[Synthesizer] G[GPS Receiver] M -->|120-240V AC| S subgraph The Box R -->|6.5V DC| V P -->|\"5V DC (USB)\"| G S -->|12V DC| C S -->|12V DC| R R -->|6.5V DC| B C -->|5V DC| P end B --->|5.5V DC| L1 B --->|5.5V DC| L2","title":"Power Distribution"},{"location":"hardware/overview/#clocks-references-and-timing","text":"flowchart BT B[FEM] V[Synthesizer] G[GPS Receiver] S[SNAP] A[GPS Antenna] subgraph The Box G -->|10 MHz| V G -->|PPS| S V -->|500 MHz| S V -->|1030 MHz| B end A --> G","title":"Clocks, References and Timing"},{"location":"hardware/overview/#monitor-and-control","text":"flowchart TB B[FEM] S[SNAP] P[Raspberry Pi] D[Server] subgraph The Box P <-->|UART| B S <-->|GPIO| P end P <--->|1 GbE| D","title":"Monitor and Control"},{"location":"software/gateware/","text":"Gateware The FPGA gateware for GReX is built using the CASPER Toolflow . Because of the reliance on proprietary tools, it is quite difficult to set up. And end-user of GReX shouldn't have to build the gateware themselves as we will provide pre-built bitstream FPG files that one can directly upload. These steps here are mostly for documentation of the gateware effort and completeness. Design Overview The gateware itself is composed of three major functional blocks: 1. Digitization 2. Channelization 3. Packetization We will go over the design and implementation of each block here. Digitization At the beginning of the signal path, the raw voltage present at the analog to digital converter's (ADC) input in converted into a signed 8 bit fixed point number with the fixed point at 7. This results in the full scale voltage data spanning from -1 to 1. As the ADCs present on the SNAP FPGA board have several inputs, we have a collection of multiplexers to select the proper pairs. The implementaion of the ADC interface in the gateware is setup such that each conversion cycle yields two samples subsequent in time. This is an artifact of the FPGA clock running at 250 MHz while the ADC is running at 500 MHz. We need the ADC to run at this speed to nyquist sample the input at 500 Msps for our 250 MHz of bandwidth. These pairs of subsequent samples need to be selected for the appropriate ADCs as well, which is the job of the second multiplexer. Both of these multiplexers are set at runtime via the standard casper interface, which will be covered later. Finally, these two sets of inputs are fed into the next stage, the F-engine. Channelization The F-Engine, or channelizer, takes both polarization's time sample pair time streams and converts them into a stream of channelized voltage data. To do so we form the \"typical\" F-Engine by using a polyphase filterbank and FFT. Together, these form a standard short-time fourier transform. Given the bandwidth of 250 MHz and our selected channel discritization of 2048 channels, we get ~122 kHz per channel every 8.192 us. The output of this F-Engine has quite high bit depth, to prevent overflows. To match our networking, we need to requantize this to something manageable. The network connection has 10 Gb/s of bandwith total, so if we resample both polarizations to 8+8 bit complex numbers, that's 16 bits per channel per polarization per 8.192 us, which is exactly 8 Gb/s. This gives us plenty of headroom and more than enough precision. To accomplish this requantization, we multiply both complex components by a user-selectable gain term and then take the 8 most significant bits. Packetization The last part of the gateware is packaing the channelized voltage data to send to the server. The way the CASPER 10 GbE block works is by accepting 64 bit words and appending them to a first-in-first-out (FIFO) buffer when a separate \"tx_valid\" signal is true. The issue here is that the clock for the 10 GbE core is external and fixed, independent of the FPGA clock. To make matters worse, it's slower than our FPGA clock of 250 MHz. So, if we clocked data in every cycle of the FPGA clock, this buffer would overflow. Not only that, but the data we have every FPGA clock cycle isn't 64 bits, it's 32 bits as we have both polarization's 8+8 bit complex channel value. So, a solution is to \"transose\" the data where every other clock cycle we create a 64 bit word of two channels worth of data. Therefore, we are clocking data in effectivley at 125 MHz, which is under the 10 GbE core clock and won't overflow. After we clock in 1024 words (as each word is 2 channels and we have 2048 channels), we must set the \"end of frame\" flag high (coincident with the last valid word). This will then transmit a UDP packet to the configured server and repeat the cycle. Timing A work in progress feature is the addition of timing information to the payloads. This will be critical eventually, but is currently untested. The SNAP board has an additional input which accepts a \"pulse per second\" signal where rising edges are coincident with a second from an accuracte UTC clock. For the case of GReX, this will come from a GPS timing reciever or external MASER source. In the gateware, there is an \"arm\" register which is logical ANDed with this PPS signal which starts the timing system. So, the user will activate this \"arm\" register and note the next UTC second (assuming the computer that is performing the arming has a reasonably accurate clock). Then, the next true second will \"flag\" the very first packet which will reset a packet counter. Every UDP packet will have a header which contains the packet count since the first PPS signal. As the clocks of the system are discriminated by this timing source, and we expect the packets to occur every 8.192 us, and we know when the UTC second of the first channel was, we can work out the timestamp of each packet. Setting up the toolflow The source of our repo will contain a git submodule that has a pinned version of the toolflow ( mlib_devel ), as to eliminate any confusion. Kiran is building the software on Arch Linux with: MATLAB R2021a Vivado 2021.1 <<<<<<< HEAD Both of these were installed directly with their installers, as these old versions don't work with the current AUR helpers (it also takes hundreds of GB of HDD space and hours of installation). ======= I used AUR helpers for both of these and played around getting their horribly vendored shared libraries to work. 3c38fdd (Add more gateware documentation)","title":"Gateware"},{"location":"software/gateware/#gateware","text":"The FPGA gateware for GReX is built using the CASPER Toolflow . Because of the reliance on proprietary tools, it is quite difficult to set up. And end-user of GReX shouldn't have to build the gateware themselves as we will provide pre-built bitstream FPG files that one can directly upload. These steps here are mostly for documentation of the gateware effort and completeness.","title":"Gateware"},{"location":"software/gateware/#design-overview","text":"The gateware itself is composed of three major functional blocks: 1. Digitization 2. Channelization 3. Packetization We will go over the design and implementation of each block here.","title":"Design Overview"},{"location":"software/gateware/#digitization","text":"At the beginning of the signal path, the raw voltage present at the analog to digital converter's (ADC) input in converted into a signed 8 bit fixed point number with the fixed point at 7. This results in the full scale voltage data spanning from -1 to 1. As the ADCs present on the SNAP FPGA board have several inputs, we have a collection of multiplexers to select the proper pairs. The implementaion of the ADC interface in the gateware is setup such that each conversion cycle yields two samples subsequent in time. This is an artifact of the FPGA clock running at 250 MHz while the ADC is running at 500 MHz. We need the ADC to run at this speed to nyquist sample the input at 500 Msps for our 250 MHz of bandwidth. These pairs of subsequent samples need to be selected for the appropriate ADCs as well, which is the job of the second multiplexer. Both of these multiplexers are set at runtime via the standard casper interface, which will be covered later. Finally, these two sets of inputs are fed into the next stage, the F-engine.","title":"Digitization"},{"location":"software/gateware/#channelization","text":"The F-Engine, or channelizer, takes both polarization's time sample pair time streams and converts them into a stream of channelized voltage data. To do so we form the \"typical\" F-Engine by using a polyphase filterbank and FFT. Together, these form a standard short-time fourier transform. Given the bandwidth of 250 MHz and our selected channel discritization of 2048 channels, we get ~122 kHz per channel every 8.192 us. The output of this F-Engine has quite high bit depth, to prevent overflows. To match our networking, we need to requantize this to something manageable. The network connection has 10 Gb/s of bandwith total, so if we resample both polarizations to 8+8 bit complex numbers, that's 16 bits per channel per polarization per 8.192 us, which is exactly 8 Gb/s. This gives us plenty of headroom and more than enough precision. To accomplish this requantization, we multiply both complex components by a user-selectable gain term and then take the 8 most significant bits.","title":"Channelization"},{"location":"software/gateware/#packetization","text":"The last part of the gateware is packaing the channelized voltage data to send to the server. The way the CASPER 10 GbE block works is by accepting 64 bit words and appending them to a first-in-first-out (FIFO) buffer when a separate \"tx_valid\" signal is true. The issue here is that the clock for the 10 GbE core is external and fixed, independent of the FPGA clock. To make matters worse, it's slower than our FPGA clock of 250 MHz. So, if we clocked data in every cycle of the FPGA clock, this buffer would overflow. Not only that, but the data we have every FPGA clock cycle isn't 64 bits, it's 32 bits as we have both polarization's 8+8 bit complex channel value. So, a solution is to \"transose\" the data where every other clock cycle we create a 64 bit word of two channels worth of data. Therefore, we are clocking data in effectivley at 125 MHz, which is under the 10 GbE core clock and won't overflow. After we clock in 1024 words (as each word is 2 channels and we have 2048 channels), we must set the \"end of frame\" flag high (coincident with the last valid word). This will then transmit a UDP packet to the configured server and repeat the cycle.","title":"Packetization"},{"location":"software/gateware/#timing","text":"A work in progress feature is the addition of timing information to the payloads. This will be critical eventually, but is currently untested. The SNAP board has an additional input which accepts a \"pulse per second\" signal where rising edges are coincident with a second from an accuracte UTC clock. For the case of GReX, this will come from a GPS timing reciever or external MASER source. In the gateware, there is an \"arm\" register which is logical ANDed with this PPS signal which starts the timing system. So, the user will activate this \"arm\" register and note the next UTC second (assuming the computer that is performing the arming has a reasonably accurate clock). Then, the next true second will \"flag\" the very first packet which will reset a packet counter. Every UDP packet will have a header which contains the packet count since the first PPS signal. As the clocks of the system are discriminated by this timing source, and we expect the packets to occur every 8.192 us, and we know when the UTC second of the first channel was, we can work out the timestamp of each packet.","title":"Timing"},{"location":"software/gateware/#setting-up-the-toolflow","text":"The source of our repo will contain a git submodule that has a pinned version of the toolflow ( mlib_devel ), as to eliminate any confusion. Kiran is building the software on Arch Linux with: MATLAB R2021a Vivado 2021.1 <<<<<<< HEAD Both of these were installed directly with their installers, as these old versions don't work with the current AUR helpers (it also takes hundreds of GB of HDD space and hours of installation). ======= I used AUR helpers for both of these and played around getting their horribly vendored shared libraries to work. 3c38fdd (Add more gateware documentation)","title":"Setting up the toolflow"},{"location":"software/gripes/","text":"Gripes This is a list of things I've figured out, but not sure what to do with yet The image that was on the RPi we've been using runs an old version of TCPBorphServer, which is incompatible with casperfpga TCOBorphServer is running through init.d and not systemd, for some reason, and all of it's logs are discarded. Why????? PSRDADA is completley undocumented The dada interface to heimdall is undocumented","title":"Gripes"},{"location":"software/gripes/#gripes","text":"This is a list of things I've figured out, but not sure what to do with yet The image that was on the RPi we've been using runs an old version of TCPBorphServer, which is incompatible with casperfpga TCOBorphServer is running through init.d and not systemd, for some reason, and all of it's logs are discarded. Why????? PSRDADA is completley undocumented The dada interface to heimdall is undocumented","title":"Gripes"},{"location":"software/guix/","text":"Pipeline Modules and Guix Guix is a functional package manager and tool to instantiate and manage Unix-like operating systems. By functional, Guix defines packages through a purely functional deployment model in which every build is deterministic and is a pure function of the package's \"inputs\" or dependencies. This solves the problem of dependency hell and reproducability. For GReX, many of our software modules and components exist as forks of preexisting software as well as some custom code. To ensure all of these components work together in harmony, we'll host a guix channel that provides the build recipes for our software here . Most of this software, however, relies on the non-free CUDA runtime. Note As a note, we are using non-free CUDA as to leverage high-performance preexisting code. CUDA, and non-free software in general, denies users the ability to study and modify it. This is detrimental to user freedom and to proper scientific review and experimentation. As such, we ask that you not share these modules widely as to encourage more open alternatives. To utilize any of our packaged software, you must add our channel to your channels.scm ( cons ( channel ( name 'guix-grex ) ( url \"https://github.com/GReX-Telescope/guix-grex.git\" ) ( branch \"main\" )) %default-channels ) Installation To start from a bare server, we need a few prerequisites. First, to actually build the install image, you need guix the guix binary installed. Installer isos will be provided, eventually. Clone the GReX Guix repo and run, this may take a while. $ guix system image --image-type = iso9660 grex/system/install.scm After that, make an installation media, either CD or USB and boot into it. If you want to make a USB, simply $ sudo dd if = <the iso that was created> of = /dev/<your USB> status = progress bs = 32M && sync Once you boot into the installation media, select \"Install using the shell based process\" Partitions We'll partition the servers with UEFI, you can use any tool you like for this, I like cfdisk . You'll need to make a 512M vfat partition for EFI and then ext4 the rest. Then, format and mount the partitions mkfs.ext4 /dev/root_partition mkfs.fat -F 32 /dev/efi_system_partition mount /dev/root_partition /mnt mkdir -p /mnt/boot/efi mount /dev/efi_system_partition /mnt/boot/efi Now we can setup the installation environment with herd start cow-store /mnt Initial Installation First, we need to grab the system configuration for this machine (assuming it exists). GReX servers we control will have their own configuration file. git clone https://github.com/GReX-Telescope/guix-grex First, we'll copy over the channels and update (this may take a while) mkdir -p ~/.config/guix Then open your editor of choice (we include vim and emacs in the installer image) and add the channels form from above to ~/.config/guix/channels.scm Then guix pull hash guix # This is necessary to ensure the updated profile path is active! Then initialize the system with cd guix-grex guix system -L . init grex/system/<specific server>.scm /mnt For example, we can provision the grex-01 server with guix system -L . init grex/system/grex-01 . scm /mnt Initial System Setup Now you can reboot and setup the users! First, we need to change the password. Login as root and passwd # For root passwd grex # For the GReX user Then, do the same steps of adding the GReX channels list to ~/.config/guix/channels.scm and then one final pull guix pull We're ready to go!","title":"Guix"},{"location":"software/guix/#pipeline-modules-and-guix","text":"Guix is a functional package manager and tool to instantiate and manage Unix-like operating systems. By functional, Guix defines packages through a purely functional deployment model in which every build is deterministic and is a pure function of the package's \"inputs\" or dependencies. This solves the problem of dependency hell and reproducability. For GReX, many of our software modules and components exist as forks of preexisting software as well as some custom code. To ensure all of these components work together in harmony, we'll host a guix channel that provides the build recipes for our software here . Most of this software, however, relies on the non-free CUDA runtime. Note As a note, we are using non-free CUDA as to leverage high-performance preexisting code. CUDA, and non-free software in general, denies users the ability to study and modify it. This is detrimental to user freedom and to proper scientific review and experimentation. As such, we ask that you not share these modules widely as to encourage more open alternatives. To utilize any of our packaged software, you must add our channel to your channels.scm ( cons ( channel ( name 'guix-grex ) ( url \"https://github.com/GReX-Telescope/guix-grex.git\" ) ( branch \"main\" )) %default-channels )","title":"Pipeline Modules and Guix"},{"location":"software/guix/#installation","text":"To start from a bare server, we need a few prerequisites. First, to actually build the install image, you need guix the guix binary installed. Installer isos will be provided, eventually. Clone the GReX Guix repo and run, this may take a while. $ guix system image --image-type = iso9660 grex/system/install.scm After that, make an installation media, either CD or USB and boot into it. If you want to make a USB, simply $ sudo dd if = <the iso that was created> of = /dev/<your USB> status = progress bs = 32M && sync Once you boot into the installation media, select \"Install using the shell based process\"","title":"Installation"},{"location":"software/guix/#partitions","text":"We'll partition the servers with UEFI, you can use any tool you like for this, I like cfdisk . You'll need to make a 512M vfat partition for EFI and then ext4 the rest. Then, format and mount the partitions mkfs.ext4 /dev/root_partition mkfs.fat -F 32 /dev/efi_system_partition mount /dev/root_partition /mnt mkdir -p /mnt/boot/efi mount /dev/efi_system_partition /mnt/boot/efi Now we can setup the installation environment with herd start cow-store /mnt","title":"Partitions"},{"location":"software/guix/#initial-installation","text":"First, we need to grab the system configuration for this machine (assuming it exists). GReX servers we control will have their own configuration file. git clone https://github.com/GReX-Telescope/guix-grex First, we'll copy over the channels and update (this may take a while) mkdir -p ~/.config/guix Then open your editor of choice (we include vim and emacs in the installer image) and add the channels form from above to ~/.config/guix/channels.scm Then guix pull hash guix # This is necessary to ensure the updated profile path is active! Then initialize the system with cd guix-grex guix system -L . init grex/system/<specific server>.scm /mnt For example, we can provision the grex-01 server with guix system -L . init grex/system/grex-01 . scm /mnt","title":"Initial Installation"},{"location":"software/guix/#initial-system-setup","text":"Now you can reboot and setup the users! First, we need to change the password. Login as root and passwd # For root passwd grex # For the GReX user Then, do the same steps of adding the GReX channels list to ~/.config/guix/channels.scm and then one final pull guix pull We're ready to go!","title":"Initial System Setup"},{"location":"software/overview/","text":"Software Overview There are two primary components to the software stack in GReX. First, the SNAP board must be configured and setup to send voltage data to the server. After that, the pipeline software should take care of the rest. This pipeline will exist as a composition of Guix packages and potentially as a Guix System definition for the entire system. This is to ensure determinisim in builds and to reduce the potential of mis-configuration. Pipeline Overview flowchart TD A[SNAP] -->|UDP| B[Byte Slurper] B -->|PSRDADA| C[Heimdall] C -->|\"?\"| D[\"?\"] Software Manifesto To limit downtime and maximize reproducability, we will try to adopt a consistent software development strategy. Primarily: Builds will be deterministic and reproducible Code will be version controlled, organized, and public Language Specific Rust No clippy warnings Avoid unsafe Document everything Formatted with rustfmt C++ Code will be formatted with clang-format's LLVM style Try to minimize (solve) all errors from -Wall Python Code will be formatted with Black Docstrings will follow the numpy format Gradual typing will be used (PEP 438) and checked with mypy or equivalent Environments will have pinned dependencies (reproducible)","title":"Overview"},{"location":"software/overview/#software-overview","text":"There are two primary components to the software stack in GReX. First, the SNAP board must be configured and setup to send voltage data to the server. After that, the pipeline software should take care of the rest. This pipeline will exist as a composition of Guix packages and potentially as a Guix System definition for the entire system. This is to ensure determinisim in builds and to reduce the potential of mis-configuration.","title":"Software Overview"},{"location":"software/overview/#pipeline-overview","text":"flowchart TD A[SNAP] -->|UDP| B[Byte Slurper] B -->|PSRDADA| C[Heimdall] C -->|\"?\"| D[\"?\"]","title":"Pipeline Overview"},{"location":"software/overview/#software-manifesto","text":"To limit downtime and maximize reproducability, we will try to adopt a consistent software development strategy. Primarily: Builds will be deterministic and reproducible Code will be version controlled, organized, and public","title":"Software Manifesto"},{"location":"software/overview/#language-specific","text":"","title":"Language Specific"},{"location":"software/overview/#rust","text":"No clippy warnings Avoid unsafe Document everything Formatted with rustfmt","title":"Rust"},{"location":"software/overview/#c","text":"Code will be formatted with clang-format's LLVM style Try to minimize (solve) all errors from -Wall","title":"C++"},{"location":"software/overview/#python","text":"Code will be formatted with Black Docstrings will follow the numpy format Gradual typing will be used (PEP 438) and checked with mypy or equivalent Environments will have pinned dependencies (reproducible)","title":"Python"},{"location":"software/pipeline/","text":"Pipeline","title":"Pipeline"},{"location":"software/pipeline/#pipeline","text":"","title":"Pipeline"},{"location":"software/snap/","text":"SNAP Configuration and Bringup The digital backend to the GReX system is a SNAP board from the CASPER group at Berkeley. This board contains the analog to digital converters and Xilinx FPGA to perform the digitization and F-engine components of the system. The setup and configuration of this board has seemingly never been well documented, so we'll try to make it as painless as possible here. The FPGA Simulink model is stored here with the latest releases found here . Grab the latest fpg file, and you're good to go - no reason to recompile it. KATCP and the Raspberry Pi The SNAP board itself has no nonvolatile memory, so every time it is power-cycled, it must be reconfigured. Rather than require the user to bring along (and know how to use) a JTAG programmer, the folks at CASPER have added a Raspsberry Pi header such that the GPIO from a Pi can bit-bang JTAG. To expose this functionality from remote devices, saving you the trouble from SSHing, they've implemented a KATCP server on the Pi known as tcpborphserver . The current source of tcpborphserver is here . KATCP is a monitor and control protocol developed by the folks at SARAO that purports easy usage and extension. Here is the list of commands they've added, which has not been updated since 2012. Raspberry Pi Setup The image for the rasperry pi comes from here . As GReX uses the RPi Model 3, grab that image and unzip it. You'll need at least a 16 GB SD card to write it to. For some reason it has a few more bytes past a standard SD card, so use something like PiShrink to resize the image to its bare minimum, then dd that, and hopefully it'll resize on boot. By default, this image sets up static networking on 192.168.0.2\\24 , so the machine its hooked up to has to talk on that same subnet. This should already be done on guix-provisioned GReX machines. KATCP Networking The Pi is (hopefully) configured to speak KATCP on 192.168.0.2:7147 . You can check this by opening a telnet connection at that address and running the ?watchdog command. You should receive a !watchdog ok response. Programming The core of the SNAP is the FPGA, whose job in GReX is to grab voltage samples from the analog to digital converters (ADCs), run them through a polyphase filterbank (PBF) to channelize, and send those channels out over the 10 GbE network interface. You would think this would be simple... <<<<<<< HEAD The \"code\" for the FPGA is stored as a bitstream binary blob (FPG). Before anything happens, this blob needs to be uploaded to the FPGA. As mentioned before, this is done over katcp using our snapctl python package which utilizes a python3 fork of casperfpga. ======= The \"code\" for the FPGA is stored as a bitstream file (FPG). How these files are created is beyond the scope of these docs, but at some point we will provide this binary blob. Before anything happens, this blob needs to be uploaded to the FPGA. As mentioned before, this is done over katcp. 3c38fdd (Add more gateware documentation) !!! TODO: Add docs on how to run this tool FPGA Clocks, References, PPS, Synthesizers The FPGA needs an external clock source, which is being provided via one of the output of the Valon synthesizer in box. Additionally, the board needs \"pulse per second\" (PPS) ticks, which come from the GPS reciever. If the user is in a place which doesn't have GPS (maybe in a lab during testing), we can override that check. TODO! Somehow TODO! Check clock is good and everything is locked? Does this happen after ADC configuration?","title":"SNAP"},{"location":"software/snap/#snap-configuration-and-bringup","text":"The digital backend to the GReX system is a SNAP board from the CASPER group at Berkeley. This board contains the analog to digital converters and Xilinx FPGA to perform the digitization and F-engine components of the system. The setup and configuration of this board has seemingly never been well documented, so we'll try to make it as painless as possible here. The FPGA Simulink model is stored here with the latest releases found here . Grab the latest fpg file, and you're good to go - no reason to recompile it.","title":"SNAP Configuration and Bringup"},{"location":"software/snap/#katcp-and-the-raspberry-pi","text":"The SNAP board itself has no nonvolatile memory, so every time it is power-cycled, it must be reconfigured. Rather than require the user to bring along (and know how to use) a JTAG programmer, the folks at CASPER have added a Raspsberry Pi header such that the GPIO from a Pi can bit-bang JTAG. To expose this functionality from remote devices, saving you the trouble from SSHing, they've implemented a KATCP server on the Pi known as tcpborphserver . The current source of tcpborphserver is here . KATCP is a monitor and control protocol developed by the folks at SARAO that purports easy usage and extension. Here is the list of commands they've added, which has not been updated since 2012.","title":"KATCP and the Raspberry Pi"},{"location":"software/snap/#raspberry-pi-setup","text":"The image for the rasperry pi comes from here . As GReX uses the RPi Model 3, grab that image and unzip it. You'll need at least a 16 GB SD card to write it to. For some reason it has a few more bytes past a standard SD card, so use something like PiShrink to resize the image to its bare minimum, then dd that, and hopefully it'll resize on boot. By default, this image sets up static networking on 192.168.0.2\\24 , so the machine its hooked up to has to talk on that same subnet. This should already be done on guix-provisioned GReX machines.","title":"Raspberry Pi Setup"},{"location":"software/snap/#katcp-networking","text":"The Pi is (hopefully) configured to speak KATCP on 192.168.0.2:7147 . You can check this by opening a telnet connection at that address and running the ?watchdog command. You should receive a !watchdog ok response.","title":"KATCP Networking"},{"location":"software/snap/#programming","text":"The core of the SNAP is the FPGA, whose job in GReX is to grab voltage samples from the analog to digital converters (ADCs), run them through a polyphase filterbank (PBF) to channelize, and send those channels out over the 10 GbE network interface. You would think this would be simple... <<<<<<< HEAD The \"code\" for the FPGA is stored as a bitstream binary blob (FPG). Before anything happens, this blob needs to be uploaded to the FPGA. As mentioned before, this is done over katcp using our snapctl python package which utilizes a python3 fork of casperfpga. ======= The \"code\" for the FPGA is stored as a bitstream file (FPG). How these files are created is beyond the scope of these docs, but at some point we will provide this binary blob. Before anything happens, this blob needs to be uploaded to the FPGA. As mentioned before, this is done over katcp. 3c38fdd (Add more gateware documentation) !!! TODO: Add docs on how to run this tool","title":"Programming"},{"location":"software/snap/#fpga-clocks-references-pps-synthesizers","text":"The FPGA needs an external clock source, which is being provided via one of the output of the Valon synthesizer in box. Additionally, the board needs \"pulse per second\" (PPS) ticks, which come from the GPS reciever. If the user is in a place which doesn't have GPS (maybe in a lab during testing), we can override that check. TODO! Somehow TODO! Check clock is good and everything is locked? Does this happen after ADC configuration?","title":"FPGA Clocks, References, PPS, Synthesizers"},{"location":"software/stack/","text":"Software Stack There are several moving parts to this whole project, most of which are organized in our GitHub organization . The notable pieces of software are: snapctl - SNAP bringup and configuration byte_slurper - UDP Packet capture and exfil to heimdal heimdall - Our fork of the pulse detection pipeline which removes clustering and RFI excision T2, T3, etc fem_mnc - Monitor and control of the FEM through the Pi's network connection These are supported by some fundamental libraries sigproc_filterbank - A rust library for reading/writing SIGPROC filterbank files psrdada-rs - A rust library for interacting with PSRDADA buffers","title":"Software Stack"},{"location":"software/stack/#software-stack","text":"There are several moving parts to this whole project, most of which are organized in our GitHub organization . The notable pieces of software are: snapctl - SNAP bringup and configuration byte_slurper - UDP Packet capture and exfil to heimdal heimdall - Our fork of the pulse detection pipeline which removes clustering and RFI excision T2, T3, etc fem_mnc - Monitor and control of the FEM through the Pi's network connection These are supported by some fundamental libraries sigproc_filterbank - A rust library for reading/writing SIGPROC filterbank files psrdada-rs - A rust library for interacting with PSRDADA buffers","title":"Software Stack"}]}